- date: Tue 01/13/26
  lecturer:
    - Arman
  title:
    - Course Introduction
    - Logistics
  readings:
  optional:
  logistics:
  slides: https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/IQAZ1Xa7F7GnRY1j3Uy8kuR2AVFetN1ZR_NEZ7fQFZ1KNcs?e=uAKDqy

- date: Thu 01/15/26
  lecturer:
  title:
    - Word embeddings and vector semantics
  readings:
    - Jurafsky & Martin Chapter 6
  optional:
  logistics:
  slides: https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/IQANiX7iVr2qQ4H2m_LrhSb-Af-uWjc7bGut2Tq7IjxxwjI?e=yMGU2X

- date: Tue 01/20/26
  lecturer:
  title:
    - Word embeddings and vector semantics (cont.)
    - Sparse representations
    - Dense representations
  readings:
    - Jurafsky & Martin Chapter 6
  optional:
    - Distributed Representation of Words and Phrases and their Compositionality (Mikolov et al., 2013) <a href="https://arxiv.org/pdf/1310.4546.pdf" target="_blank">[link]</a>
    - Efficient Estimation of Word Representations in Vector Space (Mikolov et al., 2013) <a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank">[link]</a>
    - Word2vec Explained- deriving Mikolov et al.'s negative-sampling word-embedding method (Goldberg and Levy, 2014) <a href="https://arxiv.org/pdf/1402.3722.pdf" target="_blank">[link]</a>
  logistics: HW 1 out
  slides: https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/IQAQ_f1REEz3Q7l9n4s41CgtAUGBv4O-TESG6fLHKw3Z0vU?e=TYziq6

- date: Thu 01/22/26
  lecturer:
  title:
    - Deriving the gradient of Word2vec
    - Evaluation of word embeddings
  readings:
    - Jurafsky & Martin Chapter 6
    - Distributed Representation of Words and Phrases and their Compositionality (Mikolov et al., 2013) <a href="https://arxiv.org/pdf/1310.4546.pdf" target="_blank">[link]</a>
  optional:
  logistics: 
  slides: https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/IQAFTaq2eX_mRI5pvgFVtrPkAb50jx7AfrWw33saVJtIAVs?e=BwAhKr 

- date: Tue 01/27/26
  lecturer:
  title:
    - N-Gram Language Models
    - Smoothing
    - Evaluation of Language Models
  readings:
    - Jurafsky & Martin Chapter 7
  optional:
  logistics:
  slides: https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/IQD_ZPTMLqfqRrT_yiOlFYnFAeYobXwD5EH1vmZk-I4FH3Y?e=YzEX1r

- date: Thu 01/29/26
  lecturer:
    - Arman
  title:
    - Neural network basics
    - Autograd
  readings:
    - The Matrix Calculus You Need For Deep Learning (Terence Parr and Jeremy Howard) <a href="https://arxiv.org/pdf/1802.01528.pdf" target="_blank">[link]</a>
    - Little book of deep learning (Fran√ßois Fleuret) - Ch 3, 4
  optional:
  logistics:
  slides: https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/IQC5XygBEGKiRrnDsMb2dy5LAfyY44XA652yx_YYzfKPB2g?e=SptZ2w

- date: Tue 02/03/26
  lecturer:
    - Arman
  title:
    - Auto Grad
    - Building blocks of Deep Learning for Language Modeling
    - CNNs
  readings:
    - Goldberg Chapter 9
  optional:
  logistics:

- date: Thu 02/05/26
  lecturer:
    - Arman
  title:
    - CNNs (contd.)
    - RNNs
    - Task specific neural network architectures
    - Machine translation
  readings:
    - Understanding LSTM Networks (Christopher Olah) <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">[link]</a>
    - Eisenstein, Chapter 18
  optional:
    - Neural Machine Translation and Sequence-to-sequence Models- A Tutorial (Graham Neubig) <a href="https://arxiv.org/pdf/1703.01619.pdf" target="_blank">[link]</a>
  logistics:

- date: Tue 02/10/26
  lecturer:
  title:
    - RNNs (contd.)
    - Training sequence models
    - Machine translation (contd.)
  readings:
    - Statistical Machine Translation (Koehn) <a href="https://www.statmt.org/book/" target="_blank">[link]</a>
    - Neural Machine Translation and Sequence-to-sequence Models- A Tutorial (Graham Neubig) <a href="https://arxiv.org/pdf/1703.01619.pdf" target="_blank">[link]</a>
    - Learning to Align and Translate with Attention (Bahdanau et al., 2015) <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank">[link]</a>
    - Luong et al. (2015) Effective Approaches to Attention-based Neural Machine Translation <a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank">[link]</a>
    - Attention is All You Need (Vaswani et al., 2017) <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">[link]</a>
    - Illustrated Transformer <a href="http://jalammar.github.io/illustrated-transformer/" target="_blank">[link]</a>
  logistics: Project teams due on 02/08 <br/><br/> HW1 due

- date: Thu 02/12/26
  lecturer:
  title:
    - Attention
    - Transformers
  readings:
    - Neural Machine Translation and Sequence-to-sequence Models- A Tutorial (Graham Neubig) <a href="https://arxiv.org/pdf/1703.01619.pdf" target="_blank">[link]</a>
    - Learning to Align and Translate with Attention (Bahdanau et al., 2015) <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank">[link]</a>
    - Luong et al. (2015) Effective Approaches to Attention-based Neural Machine Translation <a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank">[link]</a>
    - Attention is All You Need (Vaswani et al., 2017) <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">[link]</a>
    - Illustrated Transformer <a href="http://jalammar.github.io/illustrated-transformer/" target="_blank">[link]</a>
  optional:
  logistics: HW 2 out

- date: Tue 02/17/26
  lecturer:
    - Arman
  title:
    - Transformers (contd.)
    - Language modeling with Transformers
    - Transfer Learning
  readings:
    - Illustrated Transformer <a href="http://jalammar.github.io/illustrated-transformer/" target="_blank">[link]</a>
    - Attention is All You Need (Vaswani et al., 2017) <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">[link]</a>
    - The Annotated Transformer (Harvard NLP) <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank">[link]</a>
    - GPT-2 (Radford et al., 2019) <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank">[link]</a>
  optional:
  logistics: 

- date: Thu 02/19/26
  lecturer:
    - Arman
  title:
    - Transfer Learning (contd.)
    - Objective functions for pre-training
    - Encoder-decoder pretrained models
    - Architecture and pretraining objectives
  readings:
    - The Illustrated BERT, ELMo, and co. (Jay Alammar) <a href="http://jalammar.github.io/illustrated-bert/" target="_blank">[link]</a>
    - BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2018) <a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank">[link]</a>
    - GPT-2 (Radford et al., 2019) <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank">[link]</a>
    - T5- Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al., 2020) <a href="https://arxiv.org/pdf/1910.10683.pdf" target="_blank">[link]</a>
    - BART- Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (Lewis et al., 2019) <a href="https://arxiv.org/pdf/1910.13461.pdf" target="_blank">[link]</a>
    - What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization? (Wang et al, 2022) <a href="https://arxiv.org/abs/2204.05832" target="_blank">[link]</a>
  optional:
  logistics:

- date: Tue 02/24/26
  lecturer:
    - Arman
  title:
    - Decoding and generation
    - Large language models and impact of scale
    - In-context learning and prompting
  readings:
    - The Curious Case of Neural Text Degeneration (Holtzman et al., 2019) <a href="https://arxiv.org/pdf/1904.09751.pdf" target="_blank">[link]</a>
    - How to generate text- using different decoding methods for language generation with Transformers <a href="https://huggingface.co/blog/how-to-generate" target="_blank">[link]</a>
    - Scaling Laws for Neural Language Models (Kaplan et al., 2020) <a href="https://arxiv.org/pdf/2001.08361.pdf" target="_blank">[link]</a>
    - Training Compute-Optimal Large Language Models (Hoffmann et al., 2022) <a href="https://arxiv.org/pdf/2203.15556.pdf" target="_blank">[link]</a>
    - GPT3 paper - Language Models are Few-Shot Learners (Brown et al., 2020) <a href="https://arxiv.org/pdf/2005.14165.pdf" target="_blank">[link]</a>

- date: Thu 02/26/26
  lecturer:
    - Arman
  title: >
    <strong> Midterm Exam 1 </strong>

- date: Tue 03/03/26
  title:
    - "Guest Lecture 1: TBD"
  readings:
  optional:
  logistics: HW 2 due 

- date: Thu 03/05/26
  lecturer:
    - Arman
  title:
    - Post-training
    - Supervised Finetuning
    - Instruction Following
  readings:
    - Multitask Prompted Training Enables Zero-Shot Task Generalization (Sanh et al., 2021) <a href="https://arxiv.org/abs/2110.08207" target="_blank">[link]</a>
    - Scaling Instruction-Finetuned Language Models (Chung et al., 2022) <a href="https://arxiv.org/abs/2210.11416" target="_blank">[link]</a>
    - Are Emergent Abilities of Large Language Models a Mirage? (Sha et al., 2023) <a href="https://arxiv.org/pdf/2304.15004.pdf" target="_blank">[link]</a>
    - Emergent Abilities of Large Language Models (Wei et al., 2022) <a href="https://arxiv.org/abs/2206.07682" target="_blank">[link]</a>
  optional:
  logistics: Project proposals due 03/06

- date: 03/06/26 - 03/22/26
  title: >
    <strong> Spring recess - No classes </strong>

- date: Tue 03/24/26
  title:
    - "Guest Lecture 2: TBD"
  readings:
  optional:
  logistics:

- date: Thu 03/26/26
  title:
    - Post-training
    - Reinforcement learning from Human Feedback
    - Alignment
  readings:
    - Training language models to follow instructions with human feedback (Ouyang et al., 2022) <a href="https://arxiv.org/abs/2203.02155" target="_blank">[link]</a>
    - Fine-Tuning Language Models from Human Preferences (Ziegler et al., 2019) <a href="https://arxiv.org/abs/1909.08593" target="_blank">[link]</a>
    - Direct Preference Optimization- Your Language Model is Secretly a Reward Model (Rafailov et al., 2023) <a href="https://arxiv.org/abs/2305.18290" target="_blank">[link]</a>
    - RLAIF- Scaling Reinforcement Learning from Human Feedback with AI Feedback (Lee et al., 2023) <a href="https://arxiv.org/abs/2309.00267" target="_blank">[link]</a>
  optional:
  logistics:

- date: Tue 03/31/26
  title:
    - Post-training (contd...)
  readings:
  optional:
  logistics: HW 3 out

- date: Thu 04/02/26
  title:
    - "Guest Lecture 3: TBD"
  readings:
  optional:
  logistics:

- date: Tue 04/07/26
  lecturer:
  title:
    - Retrieval Augmented Generation (RAG)
  readings:
  optional:
  logistics:

- date: Thu 04/09/26
  lecturer:
    - Arman
  title: >
    <strong> Midterm Exam 2 </strong>

- date: Tue 04/14/26
  title:
    - "Guest Lecture 4: TBD"
  readings:
  optional:
  logistics:

- date: Thu 04/16/26
  lecturer:
  title: RAG continued, Intro to Agent-based systems
  readings:
  optional:
  logistics:

- date: Tue 04/21/26
  lecturer:
  title:
    - Project presentations session 1
  readings:
  optional:
  logistics: Final project presentations

- date: Thu 04/23/26
  lecturer:
  title: Project presentations session 2
  readings:
  optional:
  logistics: Final project presentations, HW 3 due on 4/27, <br/> Final project reports due on 4/30
